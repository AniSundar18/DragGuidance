<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>DragGuidance</title>
	<meta property="og:image" content="drag_teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Drag-Based Editing of Images through Energy Based Guidance" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Drag-Based Editing of Images through Energy Based Guidance</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://anisundar18.github.io">Anirudh Sundara Rajan</a></span>
						</center>
					</td>
	
				</tr>
			</table>
			<table align=center width=250px>
				<tr>

					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/AniSundar18/DragGuidance/tree/main'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=460px>
					<center>
						<img class="round" style="width:1000px" src="./resources/drag_teaser.png"/>
					</center>
				</td>
			</tr>
		</table>

	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
In drag-based image editing, users can select a point in an image and move it to a desired location. While repositioning the point, it is crucial to ensure that the final image remains realistic and retains likeness to the original. In this work, we leverage the principles of self-guidance to perform drag-based editing using diffusion models. By utilizing representations from the Stable Diffusion UNet, we construct energy functions that guide the generation process to generate edited images while also maintaining fidelity to the original images. 
			</td>
		</tr>
	</table>
	<br>


	<hr>

	<center><h1>Drag-Based Editing: Overview</h1></center>

<table align=center width=850px>
		<tr>
			<td>
				To perform drag-based image editing, the user has to select a point (handle point) and describe a destination (target point). Drag-based editing relies on three important steps:
				<ol>
					<li>Identifying the part of the image corresponding to the handle point.</li>
					<li>Moving the identified object as per the edit instructions.</li>
					<li>Preserving the identity of the image in other places.</li>
				</ol>
				On a high level, we first identify the part of the image that needs to be edited. Then, we define an objective to measure the progress of the edit and optimize it while preserving the remaining details of the image, resulting in an accurate edit. The specifics are provided below.
			</td>
		</tr>
</table>
	<center><h1>Step 1: Identifying the Part to be Dragged in the Image</h1></center>

<table align=center width=850px>
		<center>
		<tr>
			<td>
				We have a single point in our image as the input. We need a way to identify the regions which will also move provided that we move the point. Prior works have observed that features belonging to the same object parts have very similar representations in the later layers of the UNet. 
				<a href="https://arxiv.org/abs/2306.03881">DIFT</a> observes that the UNet in large-scale text-to-image diffusion models can learn correspondences between different images in later layers of the UNet. 
				Our key idea is simple: use the feature vector in the UNet corresponding to the handle point, and then identify the regions most similar using cosine similarity. The pipeline is detailed below:
<br><br>
<div align="center">
    For example, in the image below, the handle point of the beak in one bird can be used to detect the beak of another bird in a different position. Both images are passed through the <b>diffusion model UNet</b>, and their features are extracted. The <b>feature vector</b> corresponding to the <b>handle point</b> is selected, and the <b>element-wise similarity</b> between the target image's feature representation and this vector is computed. Applying <b>thresholding</b> reveals that the beak can be uniquely identified using this approach.
    <br><br>
    <img src="./resources/teaser_dift.png" alt="Pipeline Diagram" width="800">
</div>
			</td>
		</tr>
	</center>
</table>
<center><h1>Step 2: Dragging the Identified Part of the Image</h1></center>

<table align=center width=850px>
	<center>
		<tr>
			<td>
				Now that we have a method to identify the part of the image corresponding to the handle point, our next objective is to move this part so that the handle point aligns with the target point. To achieve this, we incorporate additional guidance terms based on the <a href="https://arxiv.org/abs/2306.00986">Self-Guidance</a> paper.
				<br><br>
				Our approach is straightforward: using the heatmap generated during the process, we calculate the average of the spatial locations, weighted by the similarity score. Intuitively, this centroid shows where the heat of the image is concentrated (and hence the position of the handle point). This calculation yields the <b>centroid</b> of the part to be moved in a differentiable manner. At the starting point, this centroid is approximately aligned with the handle point.
				<br><br>
				More formally, given a spatial similarity heat map <i>H(x, y)</i> where each pixel value represents the similarity with the handle point, we compute the <b>centroid</b> of the heat distribution using a weighted average of the spatial locations. The computation can be expressed as follows:
				<br><br>
				<div align="center" style="font-size: 1.5em;">
					<i>C = \(\frac{\sum_{x,y} (x, y) \cdot H(x, y)}{\sum_{x,y} H(x, y)}\)</i>
				</div>
				<br>
				Where:
				<ul>
					<li><b>C</b> is the centroid of the heat map.</li>
					<li><b>(x, y)</b> represents the spatial coordinates.</li>
					<li><b>H(x, y)</b> is the heat value at the spatial location <b>(x, y)</b>.</li>
				</ul>
				After computing the position of the handle point in a differentiable manner, we define a movement objective as an energy function for guidance. This function is defined as:
				<br><br>
				<div align="center" style="font-size: 1.5em;">
					<i>L<sub>movement</sub> = |C<sub>x</sub> - t<sub>x</sub>| + |C<sub>y</sub> - t<sub>y</sub>|</i>
				</div>
				<br>
				Where:
				<ul>
					<li><b>L<sub>movement</sub></b> is the L1 distance representing the movement required.</li>
					<li><b>C<sub>x</sub></b> and <b>C<sub>y</sub></b> are the coordinates of the centroid <b>C</b>.</li>
					<li><b>t<sub>x</sub></b> and <b>t<sub>y</sub></b> are the coordinates of the target point.</li>
				</ul>
			</td>
		</tr>
	</center>
</table>



	
	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

